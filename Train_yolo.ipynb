{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "!git clone https://github.com/irzaip/yolov3 yolov3\n",
    "%cd /content/yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import test  # Import test.py to get mAP after each epoch\n",
    "from models import *\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "\n",
    "# Hyperparameters: train.py --evolve --epochs 2 --img-size 320, Metrics: 0.204      0.302      0.175      0.234 (square smart)\n",
    "hyp = {'xy': 0.167,  # xy loss gain\n",
    "       'wh': 0.09339,  # wh loss gain\n",
    "       'cls': 0.03868,  # cls loss gain\n",
    "       'conf': 4.546,  # conf loss gain\n",
    "       'iou_t': 0.2454,  # iou target-anchor training threshold\n",
    "       'lr0': 0.000198,  # initial learning rate\n",
    "       'lrf': -5.,  # final learning rate = lr0 * (10 ** lrf)\n",
    "       'momentum': 0.95,  # SGD momentum\n",
    "       'weight_decay': 0.0007838}  # optimizer weight decay\n",
    "\n",
    "\n",
    "# Hyperparameters: Original, Metrics: 0.172      0.304      0.156      0.205 (square)\n",
    "# hyp = {'xy': 0.5,  # xy loss gain\n",
    "#        'wh': 0.0625,  # wh loss gain\n",
    "#        'cls': 0.0625,  # cls loss gain\n",
    "#        'conf': 4,  # conf loss gain\n",
    "#        'iou_t': 0.1,  # iou target-anchor training threshold\n",
    "#        'lr0': 0.001,  # initial learning rate\n",
    "#        'lrf': -5.,  # final learning rate = lr0 * (10 ** lrf)\n",
    "#        'momentum': 0.9,  # SGD momentum\n",
    "#        'weight_decay': 0.0005}  # optimizer weight decay\n",
    "\n",
    "# Hyperparameters: train.py --evolve --epochs 2 --img-size 320, Metrics: 0.225      0.251      0.145      0.218 (rect)\n",
    "# hyp = {'xy': 0.4499,  # xy loss gain\n",
    "#        'wh': 0.05121,  # wh loss gain\n",
    "#        'cls': 0.04207,  # cls loss gain\n",
    "#        'conf': 2.853,  # conf loss gain\n",
    "#        'iou_t': 0.2487,  # iou target-anchor training threshold\n",
    "#        'lr0': 0.0005301,  # initial learning rate\n",
    "#        'lrf': -5.,  # final learning rate = lr0 * (10 ** lrf)\n",
    "#        'momentum': 0.8823,  # SGD momentum\n",
    "#        'weight_decay': 0.0004149}  # optimizer weight decay\n",
    "\n",
    "# Hyperparameters: train.py --evolve --epochs 2 --img-size 320, Metrics: 0.178      0.313      0.167      0.212 (square)\n",
    "# hyp = {'xy': 0.4664,  # xy loss gain\n",
    "#        'wh': 0.08437,  # wh loss gain\n",
    "#        'cls': 0.05145,  # cls loss gain\n",
    "#        'conf': 4.244,  # conf loss gain\n",
    "#        'iou_t': 0.09121,  # iou target-anchor training threshold\n",
    "#        'lr0': 0.0004938,  # initial learning rate\n",
    "#        'lrf': -5.,  # final learning rate = lr0 * (10 ** lrf)\n",
    "#        'momentum': 0.9025,  # SGD momentum\n",
    "#        'weight_decay': 0.0005417}  # optimizer weight decay\n",
    "\n",
    "\n",
    "def train(\n",
    "        cfg,\n",
    "        data_cfg,\n",
    "        img_size=416,\n",
    "        resume=False,\n",
    "        epochs=273,  # 500200 batches at bs 64, dataset length 117263\n",
    "        batch_size=16,\n",
    "        accumulate=1,\n",
    "        multi_scale=False,\n",
    "        freeze_backbone=False,\n",
    "        transfer=False  # Transfer learning (train only YOLO layers)\n",
    "):\n",
    "    init_seeds()\n",
    "    weights = 'weights' + os.sep\n",
    "    latest = weights + 'latest.pt'\n",
    "    best = weights + 'best.pt'\n",
    "    device = torch_utils.select_device()\n",
    "\n",
    "    if multi_scale:\n",
    "        img_size = 608  # initiate with maximum multi_scale size\n",
    "        opt.num_workers = 0  # bug https://github.com/ultralytics/yolov3/issues/174\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True  # unsuitable for multiscale\n",
    "\n",
    "    # Configure run\n",
    "    data_dict = parse_data_cfg(data_cfg)\n",
    "    train_path = data_dict['train']\n",
    "    nc = int(data_dict['classes'])  # number of classes\n",
    "\n",
    "    # Initialize model\n",
    "    model = Darknet(cfg, img_size).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=hyp['lr0'], momentum=hyp['momentum'], weight_decay=hyp['weight_decay'])\n",
    "\n",
    "    cutoff = -1  # backbone reaches to cutoff layer\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    nf = int(model.module_defs[model.yolo_layers[0] - 1]['filters'])  # yolo layer size (i.e. 255)\n",
    "    if resume:  # Load previously saved model\n",
    "        if transfer:  # Transfer learning\n",
    "            chkpt = torch.load(weights + 'yolov3-spp.pt', map_location=device)\n",
    "            model.load_state_dict({k: v for k, v in chkpt['model'].items() if v.numel() > 1 and v.shape[0] != 255},\n",
    "                                  strict=False)\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = True if p.shape[0] == nf else False\n",
    "\n",
    "        else:  # resume from latest.pt\n",
    "            chkpt = torch.load(latest, map_location=device)  # load checkpoint\n",
    "            model.load_state_dict(chkpt['model'])\n",
    "\n",
    "        start_epoch = chkpt['epoch'] + 1\n",
    "        if chkpt['optimizer'] is not None:\n",
    "            optimizer.load_state_dict(chkpt['optimizer'])\n",
    "            best_loss = chkpt['best_loss']\n",
    "        del chkpt\n",
    "\n",
    "    else:  # Initialize model with backbone (optional)\n",
    "        if '-tiny.cfg' in cfg:\n",
    "            cutoff = load_darknet_weights(model, weights + 'yolov3-tiny.conv.15')\n",
    "        else:\n",
    "            cutoff = load_darknet_weights(model, weights + 'darknet53.conv.74')\n",
    "\n",
    "    # Scheduler https://github.com/ultralytics/yolov3/issues/238\n",
    "    # lf = lambda x: 1 - x / epochs  # linear ramp to zero\n",
    "    # lf = lambda x: 10 ** (hyp['lrf'] * x / epochs)  # exp ramp\n",
    "    lf = lambda x: 1 - 10 ** (hyp['lrf'] * (1 - x / epochs))  # inverse exp ramp\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lf, last_epoch=start_epoch - 1)\n",
    "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[218, 245], gamma=0.1, last_epoch=start_epoch-1)\n",
    "\n",
    "    # # Plot lr schedule\n",
    "    # y = []\n",
    "    # for _ in range(epochs):\n",
    "    #     scheduler.step()\n",
    "    #     y.append(optimizer.param_groups[0]['lr'])\n",
    "    # plt.plot(y, label='LambdaLR')\n",
    "    # plt.xlabel('epoch')\n",
    "    # plt.xlabel('LR')\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig('LR.png', dpi=300)\n",
    "\n",
    "    # Dataset\n",
    "    dataset = LoadImagesAndLabels(train_path, img_size, batch_size, augment=False, rect=False, image_weights=False)\n",
    "\n",
    "    # Initialize distributed training\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        dist.init_process_group(backend=opt.backend, init_method=opt.dist_url, world_size=opt.world_size, rank=opt.rank)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "        # sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "\n",
    "    # Dataloader\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=opt.num_workers,\n",
    "                            shuffle=False,  # disable rectangular training if True\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=dataset.collate_fn)\n",
    "\n",
    "    # Mixed precision training https://github.com/NVIDIA/apex\n",
    "    # install help: https://github.com/NVIDIA/apex/issues/259\n",
    "    mixed_precision = False\n",
    "    if mixed_precision:\n",
    "        from apex import amp\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "\n",
    "    # Start training\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n",
    "    model_info(model)\n",
    "    nb = len(dataloader)\n",
    "    maps = np.zeros(nc)  # mAP per class\n",
    "    results = (0, 0, 0, 0, 0)  # P, R, mAP, F1, test_loss\n",
    "    n_burnin = min(round(nb / 5 + 1), 1000)  # burn-in batches\n",
    "    for f in glob.glob('train_batch*.jpg') + glob.glob('test_batch*.jpg'):\n",
    "        os.remove(f)\n",
    "    t, t0 = time.time(), time.time()\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        print(('\\n%8s%12s' + '%10s' * 7) % ('Epoch', 'Batch', 'xy', 'wh', 'conf', 'cls', 'total', 'nTargets', 'time'))\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Freeze backbone at epoch 0, unfreeze at epoch 1\n",
    "        if freeze_backbone and epoch < 2:\n",
    "            for name, p in model.named_parameters():\n",
    "                if int(name.split('.')[1]) < cutoff:  # if layer < 75\n",
    "                    p.requires_grad = False if epoch == 0 else True\n",
    "\n",
    "        # Update image weights (optional)\n",
    "        w = model.class_weights.cpu().numpy() * (1 - maps)  # class weights\n",
    "        image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n",
    "        dataset.indices = random.choices(range(dataset.n), weights=image_weights, k=dataset.n)  # random weighted index\n",
    "\n",
    "        mloss = torch.zeros(5).to(device)  # mean losses\n",
    "        for i, (imgs, targets, _, _) in enumerate(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            nt = len(targets)\n",
    "\n",
    "            # Plot images with bounding boxes\n",
    "            if epoch == 0 and i == 0:\n",
    "                plot_images(imgs=imgs, targets=targets, fname='train_batch0.jpg')\n",
    "\n",
    "            # SGD burn-in\n",
    "            if epoch == 0 and i <= n_burnin:\n",
    "                lr = hyp['lr0'] * (i / n_burnin) ** 4\n",
    "                for x in optimizer.param_groups:\n",
    "                    x['lr'] = lr\n",
    "\n",
    "            # Run model\n",
    "            pred = model(imgs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss, loss_items = compute_loss(pred, targets, model)\n",
    "            if torch.isnan(loss):\n",
    "                print('WARNING: nan loss detected, ending training')\n",
    "                return results\n",
    "\n",
    "            # Compute gradient\n",
    "            if mixed_precision:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # Accumulate gradient for x batches before optimizing\n",
    "            if (i + 1) % accumulate == 0 or (i + 1) == nb:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Update running mean of tracked metrics\n",
    "            mloss = (mloss * i + loss_items) / (i + 1)\n",
    "\n",
    "            # Print batch results\n",
    "            s = ('%8s%12s' + '%10.3g' * 7) % (\n",
    "                '%g/%g' % (epoch, epochs - 1),\n",
    "                '%g/%g' % (i, nb - 1), *mloss, nt, time.time() - t)\n",
    "            t = time.time()\n",
    "            print(s)\n",
    "\n",
    "            # Multi-Scale training (320 - 608 pixels) every 10 batches\n",
    "            if multi_scale and (i + 1) % 10 == 0:\n",
    "                dataset.img_size = random.choice(range(10, 20)) * 32\n",
    "                print('multi_scale img_size = %g' % dataset.img_size)\n",
    "\n",
    "        # Calculate mAP (always test final epoch, skip first 5 if opt.nosave)\n",
    "        if not (opt.notest or (opt.nosave and epoch < 10)) or epoch == epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                results, maps = test.test(cfg, data_cfg, batch_size=batch_size, img_size=img_size, model=model,\n",
    "                                          conf_thres=0.1)\n",
    "\n",
    "        # Write epoch results\n",
    "        with open('results.txt', 'a') as file:\n",
    "            file.write(s + '%11.3g' * 5 % results + '\\n')  # P, R, mAP, F1, test_loss\n",
    "\n",
    "        # Update best loss\n",
    "        test_loss = results[4]\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "\n",
    "        # Save training results\n",
    "        save = (not opt.nosave) or (epoch == epochs - 1)\n",
    "        if save:\n",
    "            # Create checkpoint\n",
    "            chkpt = {'epoch': epoch,\n",
    "                     'best_loss': best_loss,\n",
    "                     'model': model.module.state_dict() if type(\n",
    "                         model) is nn.parallel.DistributedDataParallel else model.state_dict(),\n",
    "                     'optimizer': optimizer.state_dict()}\n",
    "\n",
    "            # Save latest checkpoint\n",
    "            torch.save(chkpt, latest)\n",
    "\n",
    "            # Save best checkpoint\n",
    "            if best_loss == test_loss:\n",
    "                torch.save(chkpt, best)\n",
    "\n",
    "            # Save backup every 10 epochs (optional)\n",
    "            if epoch > 0 and epoch % 10 == 0:\n",
    "                torch.save(chkpt, weights + 'backup%g.pt' % epoch)\n",
    "\n",
    "            # Delete checkpoint\n",
    "            del chkpt\n",
    "\n",
    "    dt = (time.time() - t0) / 3600\n",
    "    print('%g epochs completed in %.3f hours.' % (epoch - start_epoch, dt))\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_mutation(hyp, results):\n",
    "    # Write mutation results\n",
    "    a = '%11s' * len(hyp) % tuple(hyp.keys())  # hyperparam keys\n",
    "    b = '%11.4g' * len(hyp) % tuple(hyp.values())  # hyperparam values\n",
    "    c = '%11.3g' * len(results) % results  # results (P, R, mAP, F1, test_loss)\n",
    "    print('\\n%s\\n%s\\nEvolved fitness: %s\\n' % (a, b, c))\n",
    "    with open('evolve.txt', 'a') as f:\n",
    "        f.write(c + b + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Admin\\AppData\\Roaming\\jupyter\\runtime\\kernel-b5b222aa-7f87-4988-853f-6ef29cb80dee.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    opt.epochs = 273\n",
    "    opt.batch_size = 16\n",
    "    opt.accumulate = 1 \n",
    "    opt.cfg = 'cfg/yolov3-spp.cfg'\n",
    "    opt.data_cfg = 'data/coco.data'\n",
    "    #opt.multi-scale = 'store_true'\n",
    "    opt.img_size = 416\n",
    "    #parser.add_argument('--resume', action='store_true', help='resume training flag')\n",
    "    #parser.add_argument('--transfer', action='store_true', help='transfer learning flag')\n",
    "    opt.num_workers = 4\n",
    "    opt.dist_url = 'tcp://127.0.0.1:9999'\n",
    "    opt.rank = 0\n",
    "    opt.world_size = 1\n",
    "    opt.backend ='nccl'\n",
    "    #opt.nosave', action='store_true', help='do not save training results')\n",
    "    #parser.add_argument('--notest', action='store_true', help='only test final epoch')\n",
    "    #parser.add_argument('--evolve', action='store_true', help='run hyperparameter evolution')\n",
    "    opt.var=0\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "    print(opt)\n",
    "\n",
    "    if opt.evolve:\n",
    "        opt.notest = True  # save time by only testing final epoch\n",
    "        opt.nosave = True  # do not save checkpoints\n",
    "\n",
    "    # Train\n",
    "    results = train(\n",
    "        opt.cfg,\n",
    "        opt.data_cfg,\n",
    "        img_size=opt.img_size,\n",
    "        resume=opt.resume or opt.transfer,\n",
    "        transfer=opt.transfer,\n",
    "        epochs=opt.epochs,\n",
    "        batch_size=opt.batch_size,\n",
    "        accumulate=opt.accumulate,\n",
    "        multi_scale=opt.multi_scale,\n",
    "    )\n",
    "\n",
    "    # Evolve hyperparameters (optional)\n",
    "    if opt.evolve:\n",
    "        best_fitness = results[2]  # use mAP for fitness\n",
    "\n",
    "        # Write mutation results\n",
    "        print_mutation(hyp, results)\n",
    "\n",
    "        gen = 50  # generations to evolve\n",
    "        for _ in range(gen):\n",
    "\n",
    "            # Mutate hyperparameters\n",
    "            old_hyp = hyp.copy()\n",
    "            init_seeds(seed=int(time.time()))\n",
    "            s = [.3, .3, .3, .3, .3, .3, .3, .03, .3]\n",
    "            for i, k in enumerate(hyp.keys()):\n",
    "                x = (np.random.randn(1) * s[i] + 1) ** 1.1  # plt.hist(x.ravel(), 100)\n",
    "                hyp[k] = hyp[k] * float(x)  # vary by about 30% 1sigma\n",
    "\n",
    "            # Clip to limits\n",
    "            keys = ['lr0', 'iou_t', 'momentum', 'weight_decay']\n",
    "            limits = [(1e-4, 1e-2), (0, 0.90), (0.70, 0.99), (0, 0.01)]\n",
    "            for k, v in zip(keys, limits):\n",
    "                hyp[k] = np.clip(hyp[k], v[0], v[1])\n",
    "\n",
    "            # Determine mutation fitness\n",
    "            results = train(\n",
    "                opt.cfg,\n",
    "                opt.data_cfg,\n",
    "                img_size=opt.img_size,\n",
    "                resume=opt.resume or opt.transfer,\n",
    "                transfer=opt.transfer,\n",
    "                epochs=opt.epochs,\n",
    "                batch_size=opt.batch_size,\n",
    "                accumulate=opt.accumulate,\n",
    "                multi_scale=opt.multi_scale,\n",
    "            )\n",
    "            mutation_fitness = results[2]\n",
    "\n",
    "            # Write mutation results\n",
    "            print_mutation(hyp, results)\n",
    "\n",
    "            # Update hyperparameters if fitness improved\n",
    "            if mutation_fitness > best_fitness:\n",
    "                # Fitness improved!\n",
    "                print('Fitness improved!')\n",
    "                best_fitness = mutation_fitness\n",
    "            else:\n",
    "                hyp = old_hyp.copy()  # reset hyp to\n",
    "\n",
    "            # # Plot results\n",
    "            # import numpy as np\n",
    "            # import matplotlib.pyplot as plt\n",
    "            # a = np.loadtxt('evolve_1000val.txt')\n",
    "            # x = a[:, 2] * a[:, 3]  # metric = mAP * F1\n",
    "            # weights = (x - x.min()) ** 2\n",
    "            # fig = plt.figure(figsize=(14, 7))\n",
    "            # for i in range(len(hyp)):\n",
    "            #     y = a[:, i + 5]\n",
    "            #     mu = (y * weights).sum() / weights.sum()\n",
    "            #     plt.subplot(2, 5, i+1)\n",
    "            #     plt.plot(x.max(), mu, 'o')\n",
    "            #     plt.plot(x, y, '.')\n",
    "            #     print(list(hyp.keys())[i],'%.4g' % mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
